I"¦?<h1 id="building-a-spam-classifier">Building a Spam Classifier</h1>

<p>We all face the problem of spam e-mails in our inboxes.
Data show that the total volume of spam mails has been consistently growing in time to the point that it overtakes the amount of ordinary or <em>ham</em> emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.
Fortunately, we donâ€™t see all these messages since the providers we use already contain spam filters that simply delete or reject â€œobvious spamâ€.
However, many unwanted messages are still able to avoid these filters and make their way to us.</p>

<p>To better filter out all the unwanted spam one can build and tune an <em>ad hoc</em> filter using his own mails. This is a simple task that can be realized using some <strong>machine learning</strong> technique.
As for any machine learning project the hardest part is the preprocessing that one needs to do before training the algorithm. 
In this post I will describe in words how analyse an email dataset in order to select and extract the best features that will be be used to build a simple yet powerful spam filter.</p>

<p>My filter is written in <strong>python</strong> however the ideas I describe are very general .
For this reason I do not share any snippet in this post.
If you are interested in the code, you take a look at the Jupyter notebook in my [GitHub repo] (https://github.com/LorBordin/Spam-Classifier).</p>

<h2 id="1-getting-the-data">1. Getting the data</h2>

<p>The best data we can use to build a spam filter <strong>tuned for our (or our company) needs</strong>  are our own mails. Getting the data in this way is easy, in my case I built up a quite large database in few months.</p>

<p>For privacy reasons, I will not share my private dataset, but we will work on the <a href="https://spamassassin.apache.org/old/publiccorpus/">Apache SpamAssassinâ€™s public datasets</a>. Itâ€™s the perfect set to start with: no pre-processing at all has been made the dataset that consists in <em>raw</em> emails, the we get from our mail-box.</p>

<p>The SpamAssasinsâ€™s dataset consists in three different compressed files that contain:</p>

<ul>
  <li>2500 e-mails classified as <strong>easy ham</strong>,</li>
  <li>250 <strong>hard ham</strong> e-mails,</li>
  <li>500 <strong>spam</strong> e-mails.</li>
</ul>

<p>After having downloaded the content, the first thing we have to do before starting any analysis is to split our dataset into a <strong>train</strong> and <strong>test</strong> set.
In practice we label the emails with their class (ham or spam), then we merge the sets together, shuffle and finally split the dataset in two: a bigger one (80% of the mail) and a smaller set for the final evaluation (the test set).</p>

<h2 id="2-data-cleaning">2. Data cleaning</h2>

<p>Letâ€™s focus on the train set. We need to extract some features to train our spam-filter. A good thing to do is to look at the email content. 
Spam mails often consists in advertising messages, so they might contain some common word as <em>money</em>,  <em>opportunity</em>, <em>loan</em> or <em>mortage</em>.</p>

<p>It is therefore important to extract the words content from each mail and build 
a list with the most common spam mails.</p>

<p>Letâ€™s start with the words extraction. We need to implement the following parsing operations:</p>

<ul>
  <li>remove all the html tags,</li>
  <li>remove punctuation,</li>
  <li>covert words in lower-case,</li>
  <li>replace numbers with <em>NUMBER</em> and urls with <em>URL</em>.</li>
</ul>

<p>In python, there are some packages which are very useful for this purpose. Personally, I used <code class="highlighter-rouge">email</code> to extract the bodyâ€™s content from each email, <code class="highlighter-rouge">regex</code> and <code class="highlighter-rouge">beautifulsoup</code> to parse it.</p>

<h2 id="3-data-analysis">3. Data Analysis</h2>

<p>Once weâ€™ve extracted the words we have to <strong>count their frequency</strong>.
For this purpose letâ€™s build 2 vocabularies that count the frequency hammy and spammy words.
We do so taking care of <strong>removing the english common stopwords</strong>, i.e. words like <em>the</em>, <em>is</em>, <em>at</em>, â€¦, which are not useful to understand the purpose of an email.
Hereâ€™s the 20 most common ham and spam words in my train dataset:</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/words_freq.png" alt="" /></p>

<p>Good! Words like <strong>free</strong>, <strong>money</strong>, <strong>click</strong> looks really spammy words!</p>

<p>Not only the mailâ€™s content but also the <strong>mailâ€™s subjcet</strong> will probably contain important info.
Better make a vocabulary for spamâ€™s subject words then.
Hereâ€™s the most common spam subject 20 words:</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/subj_spam.png" alt="" /></p>

<p>Weâ€™ve already collected many interesting features, however our mails still contain a lot of info that can be useful for our filter.
For instance, letâ€™s take a look at <strong>how emails, are structured</strong>. Notice that many of them consist in multipart and, a detailed analysis reveals that spam and ham have in general a different structure:</p>

<p>![] (https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/email_type.png)</p>

<p>Thereâ€™s a lot of <em>plain text</em> in the ham dataset, while <em>html</em> is more frequent in the spam one. Good to know.</p>

<h2 id="4-preprocessing">4. Preprocessing</h2>

<p>Now that Weâ€™ve chosen the features to feed our spam-filter with we need to convert them into vectors. 
For each mail we define a vector of length 507 that stores</p>

<ul>
  <li>the frequency if the <strong>most frequent 200 ham words</strong>,</li>
  <li>the frequency of the <strong>most frequent 250 spam words</strong>,</li>
  <li>the frequency of the <strong>most frequent 50 subject words</strong>,</li>
  <li>the <strong>emailâ€™s type</strong>.</li>
</ul>

<p>Do not forget to <strong>rescale the features vector</strong> if we are going to use a Support Vector Machine or a Linear model (as I did) to fit the data.</p>

<p>We can get an idea of how the emails dispose in the features space by using the <em>t-SNE</em> algorithm that reduces the dimensionality of the features vector while trying to keep similar instances close and dissimilar instances apart.</p>

<p>![] (https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/clustering.png)</p>

<p>Notice how ham clusters in several regions, while spam clusters a bit less but lies mainly right part of the plot.</p>

<p>Before proceeding to the training part. letâ€™s tackle the problem of <strong>overfitting</strong>. Our features vector has a large number of entries (507), moreover many of them are words that appear both in the ham and the spam vocabularies, so there are many repeated entries. If we train an algorithm with this vector most probably we are going to overfit it! To avoid this problem we define a function that randomly selects only a subset of entries, taking care of avoiding repeated features. In this way each classifier weâ€™re going to use, will be trained on a different set of features.</p>

<h2 id="5-training-some-algorithm">5. Training some algorithm</h2>

<p>Time to train our filter!</p>

<p>But firstly we need to choose a <strong>training score</strong>. 
The simplest thing we could be tempted to use is the <strong>accuracy score</strong> that just counts the number of wrong predictions</p>

<script type="math/tex; mode=display">accuracy = \frac{wrong\ predictions}{total}\,.</script>

<p>This is <strong>not a good score</strong>, though. 
In fact, in our dataset only $ \sim 15\%$ of mails are spam. So, a <em>dumb filter</em> that classifies everything as ham would score an accuracy of $ \sim 85\%$ which is already high.<br />
Moreover, accuracy considers <em>false positive</em> (FP) and <em>false negative</em> (FN) equally important. In other words it makes no distinction if ham is classified as spam (FP), or if spam is classified as ham (FN). But of course we care more that our spam filter does not filter out ham messages, better if it lets some spam pass instead!
This is to say that instead of accuracy itâ€™s better to use a combination of two additional scores, <strong>precision</strong> and <strong>recall</strong>,</p>

<script type="math/tex; mode=display">precision = \frac{TP}{TP+FP}\,, \qquad\qquad recall = \frac{TP}{TP+FN}\,,</script>

<p>where TP and FP stand respectively for <em>true positive</em> and <em>false negative</em>, which are respectively, spam and spam classified as ham. Ideally we want to tune our filter to have the maximum possible precision, i.e. 1, and the highest recall.
The <em>harmonic mean</em> of precision and recall is called <strong>$F_1$ score</strong> and works better than accuracy. This is the score we are going to maximize in training the spam filter.</p>

<p>We chose the score, letâ€™s train the classifier! But..<br />
What model shall we use? Linear model? Support Vector Machine? Random Forest? Well letâ€™s train them all and <strong>make a voting classifier</strong>, most probably itâ€™ll perform better than any individual classifier.
Personally, Iâ€™ve trained the following algorithms:</p>

<ul>
  <li>a <strong>Logistic regressor</strong>,</li>
  <li>a Stochastic Gradient Descent or <strong>SGD classifier</strong>,</li>
  <li>a <strong>Random Forest classifier</strong>,</li>
  <li>a <strong>K Neighbors classifier</strong>,</li>
  <li>a Support Vector Machine or <strong>SVM classifer</strong>.</li>
</ul>

<p>As discussed in the previous section, itâ€™s better to train each classifier with different features sets, smaller than the total features set. This will help not only for lowering the risk of overfitting, but in this way the algorithms will make uncorrelated errors and the voting classifier will perform better.</p>

<p>To get an idea of how the individual classifiers will generalize to an independent data set we use <em>cross-validation</em>. In practice, we  divide the train dataset in 5 validation sets, then the classifier is evaluated on each validation set, after it is trained on the rest of the data.
Hereâ€™s the results of each model:</p>

<div align="center">
<img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/cv_performances.png" width="400" />
</div>

<p>Since the Random Forest classifier performs the best among the individual ones, its vote count twice on the voting classifier.</p>

<p>A great quality of Random Forests is that they make it easy to measure the relative importance of each feature. The following plot shows the features that contributed more than $1\%$ in the decision process.</p>

<p>![] (https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/feature_importances.png)</p>

<p>It seems that the most important feature is the word <em>click</em> ($8 \%$) followed by <em>please</em> and the email types <em>text/html</em> and <em>text/plain</em> (all around $ 5 \%$).</p>

<h2 id="6-final-evaluation">6 Final evaluation</h2>

<p>Weâ€™ve fine-tuned and trained the algorithms, cross validated them on the train dataset and built a voting classifier. Itâ€™s time to check the performances on the test dataset to asses the performances of our spam filter!</p>

<p>Of course we firstly need to preprocess the test set generating the features vectors as we did for the training set. Then we can finally asses how the voting classifier behaves on an unknown dataset.</p>

<div align="center">
 <img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_voting.png" width="200" />
</div>

<p>It performs very well, confirming the results of the cross-validation test!
Letâ€™s take a look at the <strong>confusion matrix</strong> :</p>

<div align="center">
 <img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/confusion_matrix.png" width="300" />
</div>

<p>Wow! It misclassified only 2 ham mails; at the same time it let passed 13 spam mails out of 100. Not bad!</p>

<p>But letâ€™s take a closer look at the misclassified ham mails:</p>

<p>The first is an advertising email form Ryanair</p>

<blockquote>
  <p>Ryanair in partnership with Primary Insurance
offer excellent value travel insurance from
Â£7.00GBP/9.00 Euro per person for 31 day cover.</p>

  <p>Annual travel insurance* from Â£45.00GBP/63.00 Euro, includes 24 days winter
sports cover !</p>

  <p>Our travel insurance provides a high standard of cover.</p>

  <p>Summary of Cover</p>

  <p>Medical Expenses up to Â£2 million</p>

  <p>Personal Liability  up to Â£2 million<br />
Personal Effects &amp; Baggage up to Â£750<br />
Personal Accident Maximum Benefit Â£15,000</p>

  <p>â€¦</p>
</blockquote>

<p>while the second is written in Japanese (according to Google translate)</p>

<blockquote>
  <p>OpenTextç¤¾<br />
ä¼Šæ±æ§˜</p>

  <p>ã„ã¤ã‚‚ãŠä¸–è©±ã«ãªã£ã¦ãŠã‚Šã¾ã™ã€‚ 
å®‰äº•@infocomã§ã™ã€‚</p>

  <p>ã‚ã‚‹ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã«ã€é©å½“ãªãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã‚’è¤‡æ•°ã€ã‚¿ã‚¹ã‚¯ã‚’è¤‡æ•°ç”¨æ„ã—<br />
ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã¯ã€å­˜åœ¨ã™ã‚‹é©å½“ãªãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã«å‰²ã‚Šå½“ã¦ãŸã‚‚ã®ã¨<br />
ã—ã¾ã™ã€‚ 
ã‚ã‚‹ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã‚’è¦‹ã‚‹ã¨ã€æ·»ä»˜ã®çµµã«ã‚ã‚‹ã‚ˆã†ãªç”»é¢ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã§ã™ãŒ<br />
ã“ã“ã§ä¸¸ã§å›²ã£ãŸéƒ¨åˆ†ï¼ˆ æœŸé–“ ï¼‰ã¯ã€ä½•ã‚’æ„å‘³ã—ã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ</p>

  <p>ã¤ã¾ã‚Šã€ï¼“ï¼—æ—¥ ã€ï¼’ï¼—æ—¥<br />
ã®ãã‚Œãã‚Œã®æ„å‘³ï¼ˆç®—å‡ºã®ã•ã‚Œæ–¹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ï¼‰</p>

  <p>ã¾ãŸã€Œé€±æœ«ã‚’é™¤ãã€ã¨ã‚ã‚Šã¾ã™ãŒã€é€±æœ«ã‚’é™¤ã‹ãªã„è¨­å®šæ–¹æ³•ã¯ã‚ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ</p>

  <p>ä»¥ä¸Šã€ã”å›ç­”ã®æ–¹ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚</p>

  <p>â€“
+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“+<br />
ã‚¤ãƒ³ãƒ•ã‚©ã‚³ãƒ (æ ª)<br />
ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆæœ¬éƒ¨<br />
KMãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢éƒ¨<br />
ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã‚°ãƒ«ãƒ¼ãƒ—</p>

  <p>ã€’101-0062
æ±äº¬éƒ½åƒä»£ç”°åŒºç¥ç”°é§¿æ²³å°3-11ä¸‰äº•ä½å‹æµ·ä¸Šé§¿æ²³å°åˆ¥é¤¨5F<br />
å®‰äº•  å‰›<br />
E-mail: go@infocom.co.jp<br />
TEL: 03-3518-3299<br />
FAX: 03-3518-3055</p>
</blockquote>

<p>Here, the distinction between ham and spam gets thin, personally Iâ€™d labelled both the emails as spam.</p>

<p>Before concluding, letâ€™s compare the performances of the individual classifiers on the test set:</p>

<div align="center">
<img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_performances.png" width="400" />
</div>

<p>This confirms that <strong>the voting classifier scores better than any individual one!</strong></p>

<h2 id="7-conclusion">7. Conclusion</h2>

<p>In this post weâ€™ve exploited a public dataset of raw emails and in order to make a <strong>simple yet powerful spam-filter</strong> that detects most of the spam that evade the controls of our providers.</p>

<p>Again, take a look at my GitHub repo (MAKE LINK) and in particular at the  <strong>Jupyter Notebook</strong> if you wanna inspect the code I used to analyze and preprocess the mails and to train the filter.</p>

<!--- comment on periodically update the dictionaries and re-train the voting classifier in order to keep the filter up-to-date with the new spam e-mails. 
This is a simple task and it isn't time consuming as well. --->

<p>Finally, Iâ€™d like to comment about possible extensions. 
Weâ€™ve developed a spam filter, however the same ideas can applied to other similar tasks; the code as well could be used with little changes.</p>

<p>Obvious examples are a <strong>Hate-speach detector</strong>, that classifies profiles in a social network with little modifications, or a <strong>Clickbait recognizer</strong>, that detects false advertisement from a website.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>[Wikipedia/spam] (https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates)Â <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET