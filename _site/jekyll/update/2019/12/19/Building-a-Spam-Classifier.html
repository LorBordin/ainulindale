<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Building a Spam Classifier | Ainulindalë</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Building a Spam Classifier" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We all face the problem of spam e-mails in our inboxes. Data show that the total volume of spam mails has been consistently growing in time to the point that it overtakes the amount of ordinary or ham emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted 1. Fortunately, we don’t see all these messages since the providers we use already contain spam filters that simply delete or reject obvious spam. However, many unwanted messages are still able to avoid these filters and make their way to us. [Wikipedia/spam] (https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates”&gt;Wikipedia/spam) &#8617;" />
<meta property="og:description" content="We all face the problem of spam e-mails in our inboxes. Data show that the total volume of spam mails has been consistently growing in time to the point that it overtakes the amount of ordinary or ham emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted 1. Fortunately, we don’t see all these messages since the providers we use already contain spam filters that simply delete or reject obvious spam. However, many unwanted messages are still able to avoid these filters and make their way to us. [Wikipedia/spam] (https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates”&gt;Wikipedia/spam) &#8617;" />
<link rel="canonical" href="http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html" />
<meta property="og:url" content="http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html" />
<meta property="og:site_name" content="Ainulindalë" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-19T00:00:00+00:00" />
<script type="application/ld+json">
{"description":"We all face the problem of spam e-mails in our inboxes. Data show that the total volume of spam mails has been consistently growing in time to the point that it overtakes the amount of ordinary or ham emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted 1. Fortunately, we don’t see all these messages since the providers we use already contain spam filters that simply delete or reject obvious spam. However, many unwanted messages are still able to avoid these filters and make their way to us. [Wikipedia/spam] (https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates”&gt;Wikipedia/spam) &#8617;","@type":"BlogPosting","url":"http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html","headline":"Building a Spam Classifier","dateModified":"2019-12-19T00:00:00+00:00","datePublished":"2019-12-19T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ainulindale/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/ainulindale/feed.xml" title="Ainulindalë" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/ainulindale/">Ainulindalë</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Building a Spam Classifier</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-12-19T00:00:00+00:00" itemprop="datePublished">Dec 19, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>We all face the problem of spam e-mails in our inboxes.
Data show that the total volume of spam mails has been consistently growing in time to the point that it overtakes the amount of ordinary or <em>ham</em> emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted <sup id="fnref:foot"><a href="#fn:foot" class="footnote">1</a></sup>.
Fortunately, we don’t see all these messages since the providers we use already contain spam filters that simply delete or reject <em>obvious spam</em>.
However, many unwanted messages are still able to avoid these filters and make their way to us.</p>

<p>To better filter out all the unwanted spam one can build and tune an <em>ad hoc</em> filter using his own mails. This is a simple task that can be realized using some <strong>machine learning</strong> technique.
As for any machine learning project the hardest part is the preprocessing that one needs to do before training the algorithm. 
In this post I will describe in words how analyse an email dataset in order to select and extract the best features that will be be used to build a simple yet powerful spam filter.</p>

<p>My filter is written in <strong>python</strong> however the ideas I describe are very general .
For this reason I do not share any snippet in this post.
If you are interested in the code, you take a look at the Jupyter notebook in my [GitHub repo] (https://github.com/LorBordin/Spam-Classifier).</p>

<p>##1. Getting the data</p>

<p>The best data we can use to build a spam filter <strong>tuned for our (or our company) needs</strong>  are our own mails. Getting the data in this way is easy, in my case I built up a quite large database in few months.</p>

<p>For privacy reasons, I will not share my private dataset, but we will work on the <a href="https://spamassassin.apache.org/old/publiccorpus/">Apache SpamAssassin’s public datasets</a>. It’s the perfect set to start with: no pre-processing at all has been made the dataset that consists in <em>raw</em> emails, the we get from our mail-box.</p>

<p>The SpamAssasins’s dataset consists in three different compressed files that contain:</p>

<ul>
  <li>2500 e-mails classified as <strong>easy ham</strong>,</li>
  <li>250 <strong>hard ham</strong> e-mails,</li>
  <li>500 <strong>spam</strong> e-mails.</li>
</ul>

<p>After having downloaded the content, the first thing we have to do before starting any analysis is to split our dataset into a <strong>train</strong> and <strong>test</strong> set.
In practice we label the emails with their class (ham or spam), then we merge the sets together, shuffle and finally split the dataset in two: a bigger one (80% of the mail) and a smaller set for the final evaluation (the test set).</p>

<h2 id="2-data-cleaning">2. Data cleaning</h2>

<p>Let’s focus on the train set. We need to extract some features to train our spam-filter. A good thing to do is to look at the email content. 
Spam mails often consists in advertising messages, so they might contain some common word as <em>money</em>,  <em>opportunity</em>, <em>loan</em> or <em>mortage</em>.</p>

<p>It is therefore important to extract the words content from each mail and build 
a list with the most common spam mails.</p>

<p>Let’s start with the words extraction. We need to implement the following parsing operations:</p>

<ul>
  <li>remove all the html tags,</li>
  <li>remove punctuation,</li>
  <li>covert words in lower-case,</li>
  <li>replace numbers with <em>NUMBER</em> and urls with <em>URL</em>.</li>
</ul>

<p>In python, there are some packages which are very useful for this purpose. Personally, I used  <code class="highlighter-rouge">email </code> to extract the body’s content from each email,  <code class="highlighter-rouge">regex </code> and  <code class="highlighter-rouge">beautifulsoup </code> to parse it.</p>

<h2 id="3-data-analysis">3. Data Analysis</h2>

<p>Once we’ve extracted the words we have to <strong>count their frequency</strong>.
For this purpose let’s build 2 vocabularies that count the frequency hammy and spammy words.
We do so taking care of <strong>removing the english common stopwords</strong>, i.e. words like <em>the</em>, <em>is</em>, <em>at</em>, …, which are not useful to understand the purpose of an email.
Here’s the 20 most common ham and spam words in my train dataset:</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/words_freq.png" alt="" /></p>

<p>Good! Words like <strong>free</strong>, <strong>money</strong>, <strong>click</strong> looks really spammy words!</p>

<p>Not only the mail’s content but also the <strong>mail’s subjcet</strong> will probably contain important info.
Better make a vocabulary for spam’s subject words then.
Here’s the most common spam subject 20 words:</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/subj_spam.png" alt="" /></p>

<p>We’ve already collected many interesting features, however our mails still contain a lot of info that can be useful for our filter.
For instance, let’s take a look at <strong>how emails, are structured</strong>. Notice that many of them consist in multipart and, a detailed analysis reveals that spam and ham have in general a different structure:</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/email_type.png" alt="" /></p>

<p>There’s a lot of <em>plain text</em> in the ham dataset, while <em>html</em> is more frequent in the spam one. Good to know.</p>

<h2 id="4-preprocessing">4. Preprocessing</h2>

<p>Now that We’ve chosen the features to feed our spam-filter with we need to convert them into vectors. 
For each mail we define a vector of length 507 that stores</p>

<ul>
  <li>the frequency if the <strong>most frequent 200 ham words</strong>,</li>
  <li>the frequency of the <strong>most frequent 250 spam words</strong>,</li>
  <li>the frequency of the <strong>most frequent 50 subject words</strong>,</li>
  <li>the <strong>email’s type</strong>.</li>
</ul>

<p>Do not forget to <strong>rescale the features vector</strong> if we are going to use a Support Vector Machine or a Linear model (as I did) to fit the data.</p>

<p>We can get an idea of how the emails dispose in the features space by using the <em>t-SNE</em> algorithm that reduces the dimensionality of the features vector while trying to keep similar instances close and dissimilar instances apart.</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/clustering.png" alt="" /></p>

<p>Notice how ham clusters in several regions, while spam clusters a bit less but lies mainly right part of the plot.</p>

<p>Before proceeding to the training part. let’s tackle the problem of <strong>overfitting</strong>. Our features vector has a large number of entries (507), moreover many of them are words that appear both in the ham and the spam vocabularies, so there are many repeated entries. If we train an algorithm with this vector most probably we are going to overfit it! To avoid this problem we define a function that randomly selects only a subset of entries, taking care of avoiding repeated features. In this way each classifier we’re going to use, will be trained on a different set of features.</p>

<h2 id="5-training-some-algorithm">5. Training some algorithm</h2>

<p>Time to train our filter!</p>

<p>But firstly we need to choose a <strong>training score</strong>. 
The simplest thing we could be tempted to use is the <strong>accuracy score</strong> that just counts the number of wrong predictions</p>

<script type="math/tex; mode=display">accuracy = \frac{wrong\ predictions}{total}\,.</script>

<p>This is <strong>not a good score</strong>, though. 
In fact, in our dataset only ( \sim 15\%) of mails are spam. So, a <em>dumb filter</em> that classifies everything as ham would score an accuracy of ( \sim 85\%) which is already high.<br />
Moreover, accuracy considers <em>false positive</em> (FP) and <em>false negative</em> (FN) equally important. In other words it makes no distinction if ham is classified as spam (FP), or if spam is classified as ham (FN). But of course we care more that our spam filter does not filter out ham messages, better if it lets some spam pass instead!
This is to say that instead of accuracy it’s better to use a combination of two additional scores, <strong>precision</strong> and <strong>recall</strong>,</p>

<script type="math/tex; mode=display">precision = \frac{TP}{TP+FP}\,, \qquad\qquad recall = \frac{TP}{TP+FN}\,,</script>

<p>where TP and FP stand respectively for <em>true positive</em> and <em>false negative</em>, which are respectively, spam and spam classified as ham. Ideally we want to tune our filter to have the maximum possible precision, i.e. 1, and the highest recall.
The <em>harmonic mean</em> of precision and recall is called <strong>$F_1$ score</strong> and works better than accuracy. This is the score we are going to maximize in training the spam filter.</p>

<p>We chose the score, let’s train the classifier! But..<br />
What model shall we use? Linear model? Support Vector Machine? Random Forest? Well let’s train them all and <strong>make a voting classifier</strong>, most probably it’ll perform better than any individual classifier.
Personally, I’ve trained the following algorithms:</p>

<ul>
  <li>a <strong>Logistic regressor</strong>, - a Stochastic Gradient Descent or <strong>SGD classifier</strong>,</li>
  <li>a <strong>Random Forest classifier</strong>,</li>
  <li>a <strong>K Neighbors classifier</strong>,</li>
  <li>a Support Vector Machine or <strong>SVM classifer</strong>.</li>
</ul>

<p>As discussed in the previous section, it’s better to train each classifier with different features sets, smaller than the total features set. This will help not only for lowering the risk of overfitting, but in this way the algorithms will make uncorrelated errors and the voting classifier will perform better.</p>

<p>To get an idea of how the individual classifiers will generalize to an independent data set we use <em>cross-validation</em>. In practice, we  divide the train dataset in 5 validation sets, then the classifier is evaluated on each validation set, after it is trained on the rest of the data.
Here’s the results of each model:</p>

<div align="center">
<img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/cv_performances.png" width="400" />
</div>

<p>Since the Random Forest classifier performs the best among the individual ones, its vote count twice on the voting classifier.</p>

<p>A great quality of Random Forests is that they make it easy to measure the relative importance of each feature. The following plot shows the features that contributed more than 1% in the decision process.</p>

<p><img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/feature_importances.png" alt="" /></p>

<p>It seems that the most important feature is the word <em>click</em> (8 %) followed by <em>please</em> and the email types <em>text/html</em> and <em>text/plain</em> (all around 5%).</p>

<h2 id="6-final-evaluation">6. Final evaluation</h2>

<p>We’ve fine-tuned and trained the algorithms, cross validated them on the train dataset and built a voting classifier. It’s time to check the performances on the test dataset to asses the performances of our spam filter!</p>

<p>Of course we firstly need to preprocess the test set generating the features vectors as we did for the training set. Then we can finally asses how the voting classifier behaves on an unknown dataset.</p>

<div align="center">
 <img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_voting.png" width="200" />
</div>

<p>It performs very well, confirming the results of the cross-validation test!
Let’s take a look at the <strong>confusion matrix</strong> :</p>

<div align="center">
 <img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/confusion_matrix.png" width="300" />
</div>

<p>Wow! It misclassified only 2 ham mails; at the same time it let passed 13 spam mails out of 100. Not bad!</p>

<p>But let’s take a closer look at the misclassified ham mails:</p>

<p>The first is an advertising email form Ryanair</p>

<blockquote>
  <p>Ryanair in partnership with Primary Insurance<br />
offer excellent value travel insurance from<br />
£7.00GBP/9.00 Euro per person for 31 day cover.</p>

  <p>Annual travel insurance* from £45.00GBP/63.00 Euro,<br />
includes 24 days winter sports cover !</p>

  <p>Our travel insurance provides a high standard of cover.</p>

  <p>Summary of Cover</p>

  <p>Medical Expenses up to £2 million<br />
Personal Liability  up to £2 million <br />
Personal Effects &amp; Baggage up to £750<br />
Personal Accident Maximum Benefit £15,000</p>

  <p>…</p>
</blockquote>

<p>while the second is written in Japanese (according to Google translate)</p>

<blockquote>
  <p>OpenText社<br />
伊東様</p>

  <p>いつもお世話になっております。 
安井@infocomです。</p>

  <p>あるタスクリストに、適当なマイルストーンを複数、タスクを複数用意し<br />
それぞれのタスクについては、存在する適当なマイルストーンに割り当てたものと<br />
します。<br />
あるマイルストーンを見ると、添付の絵にあるような画面が表示されるのですが<br />
ここで丸で囲った部分（ 期間 ）は、何を意味しているのでしょうか？</p>

  <p>つまり、３７日 、２７日<br />
のそれぞれの意味（算出のされ方について教えてください。）</p>

  <p>また「週末を除く」とありますが、週末を除かない設定方法はあるのでしょうか？</p>

  <p>以上、ご回答の方よろしくお願いいたします。</p>

  <p>–
+————————————–+
インフォコム(株)<br />
ナレッジマネジメント本部<br />
KMフロンティア部<br />
コラボレーティブシステムグループ</p>

  <p>〒101-0062<br />
東京都千代田区神田駿河台3-11三井住友海上駿河台別館5F<br />
安井  剛<br />
E-mail: go@infocom.co.jp<br />
TEL: 03-3518-3299<br />
FAX: 03-3518-3055</p>
</blockquote>

<p>Here, the distinction between ham and spam gets thin, personally I’d labelled both the emails as spam.</p>

<p>Before concluding, let’s compare the performances of the individual classifiers on the test set:</p>

<div align="center">
<img src="https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_performances.png" width="400" />
</div>

<p>This confirms that <strong>the voting classifier scores better than any individual one!</strong></p>

<h2 id="7-conclusion">7. Conclusion</h2>

<p>In this post we’ve exploited a public dataset of raw emails and in order to make a <strong>simple yet powerful spam-filter</strong> that detects most of the spam that evade the controls of our providers.</p>

<p>Again, take a look at my GitHub repo (MAKE LINK) and in particular at the  <strong>Jupyter Notebook</strong> if you wanna inspect the code I used to analyze and preprocess the mails and to train the filter.</p>

<!--- comment on periodically update the dictionaries and re-train the voting classifier in order to keep the filter up-to-date with the new spam e-mails. 
This is a simple task and it isn't time consuming as well. --->

<p>Finally, I’d like to comment about possible extensions. 
We’ve developed a spam filter, however the same ideas can applied to other similar tasks; the code as well could be used with little changes.</p>

<p>Obvious examples are a <strong>Hate-speach detector</strong>, that classifies profiles in a social network with little modifications, or a <strong>Clickbait recognizer</strong>, that detects false advertisement from a website.</p>

<div class="footnotes">
  <ol>
    <li id="fn:foot">
      <p>[Wikipedia/spam] (https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates”&gt;Wikipedia/spam) <a href="#fnref:foot" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ainulindale/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ainulindalë</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ainulindalë</li><li><a class="u-email" href="mailto:lorenzo.bordin@nottingham.ac.uk">lorenzo.bordin@nottingham.ac.uk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/LorBordin"><svg class="svg-icon"><use xlink:href="/ainulindale/assets/minima-social-icons.svg#github"></use></svg> <span class="username">LorBordin</span></a></li><li><a href="https://www.linkedin.com/in/lorenzo-bordin-694b6812a"><svg class="svg-icon"><use xlink:href="/ainulindale/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">lorenzo-bordin-694b6812a</span></a></li><li><a href="https://www.twitter.com/None"><svg class="svg-icon"><use xlink:href="/ainulindale/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">None</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is Lorenzo&#39;s blog, Tolkien appasionate and theoretical phisicist. After 6 years in the academia he is now a  data scientist.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
