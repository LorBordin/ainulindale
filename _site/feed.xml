<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/ainulindale/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/ainulindale/" rel="alternate" type="text/html" /><updated>2020-01-29T14:11:23+00:00</updated><id>http://localhost:4000/ainulindale/feed.xml</id><title type="html">Ainulindalë</title><subtitle>I’m Lorenzo, a theoretical physicist working in Cosmology at the University of Nottingham.  Data science and Machine Learning appassionate, I’ve created this blog to share what I’ve learned in this field.</subtitle><entry><title type="html">3 Things on Clickbait You’ll Never Believe</title><link href="http://localhost:4000/ainulindale/jekyll/update/2020/01/23/Three-facts-on-Clickbait-you'll-never-believe.html" rel="alternate" type="text/html" title="3 Things on Clickbait You'll Never Believe" /><published>2020-01-23T00:00:00+00:00</published><updated>2020-01-23T00:00:00+00:00</updated><id>http://localhost:4000/ainulindale/jekyll/update/2020/01/23/Three-facts-on-Clickbait-you'll-never-believe</id><content type="html" xml:base="http://localhost:4000/ainulindale/jekyll/update/2020/01/23/Three-facts-on-Clickbait-you'll-never-believe.html">&lt;p&gt;&lt;strong&gt;Clickbaiting&lt;/strong&gt; is the intenctional act of over-promising or otherwise misrepresenting what you’re going to find when you read a story on the web.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/clickbt_ex_1.png&quot; width=&quot;200&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/clickbt_ex_2.png&quot; width=&quot;180&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/clickbt_ex_3.png&quot; width=&quot;190&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Headlines, like the one above, work on our brain in a very subtle way. Their aim is to exploit the &lt;em&gt;curiosity gap&lt;/em&gt;, providing just enough information to make the reader curious, but not enough to satisfy her/his curiosity without clicking through to the linked content.&lt;/p&gt;

&lt;p&gt;Over time, clickbaiting can get potentially dangerous. 
Studies show that not only readers get addicited to captivating headlines, but they  prevent people to come up with their own mental models.
In addition to these, clickbait articles increseangly often do not deliver any substance and because of their catchy titles, usually very emotionals, they tend to make readers less objective.&lt;/p&gt;

&lt;p&gt;So if they are so distruptive in nature, why are they so used? Of course, there’s only one reason for that: &lt;strong&gt;money&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On-line news websites are competing for your attention because they need it to thrive in today’s economy. Our interactions on these platforms generate invaluable data without which the machine intelligence that makes them so intuitive and personalized would fail to exist.&lt;/p&gt;

&lt;p&gt;All this comes at the expense of the depletion of real journalism. Important and renowned newspapers have starded to produce clickbait contents as well.&lt;/p&gt;

&lt;p&gt;To better understand this phenomenon in journalism I have decided to carry out a quantitative analysis to see how much clickbait affects ordinary journalism. 
I’ve selected headlines from some of the most famous US and UK newspapers.
Data show the raising of the clickbait phenomenon over time.&lt;/p&gt;

&lt;p&gt;Stay tuned, the results I found will shock you!&lt;/p&gt;

&lt;h2 id=&quot;a-clickbait-detector&quot;&gt;A Clickbait Detector&lt;/h2&gt;

&lt;p&gt;Classifing tousands of headlines would not be possible without the help of the artificial intelligence. 
Therefore, I’ve firstly developed a &lt;strong&gt;Clickbait detector&lt;/strong&gt; that uses &lt;strong&gt;machine learning&lt;/strong&gt;  to classify headlines.&lt;/p&gt;

&lt;p&gt;As training data I’ve used the &lt;strong&gt;AOSSIE Click Bait Dataset&lt;/strong&gt;, publicly available on &lt;a href=&quot;https://www.kaggle.com/ad6398/aossie-click-bait-dataset&quot;&gt;Kaggle&lt;/a&gt;: it consists in a huge set of labelled news titles, collected from variuos websites like &lt;em&gt;Buzzfeed&lt;/em&gt; or The &lt;em&gt;New York Times&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Inspired by this research paper, &lt;a href=&quot;https://arxiv.org/abs/1610.09786&quot;&gt;Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media&lt;/a&gt;, I’ve used some NLP tools to extract the following training features from each headline:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lemmatised words count,&lt;/li&gt;
  &lt;li&gt;lenght of title,&lt;/li&gt;
  &lt;li&gt;stopwords ratio,&lt;/li&gt;
  &lt;li&gt;contractions ratio.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, I’ve trained a Random Forest Classifier. 
If you’re courious to see how the detector works, take a look at my code! Here’s the &lt;a href=&quot;https://github.com/LorBordin/clickbait_analysis&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-raising-of-clickbait-over-time&quot;&gt;The Raising of Clickbait Over Time&lt;/h2&gt;

&lt;p&gt;I begun my analysis studing the trend of clickbait headlines over time. I’ve used as database the one provided by the &lt;em&gt;New York Times&lt;/em&gt; (NYT) Archive API. 
It provides data of all the articles that appeard in the NYT since 1851.&lt;/p&gt;

&lt;p&gt;The API makes collecting headlines relatively easy. 
I’ve collected all the those entries with the attribute &lt;code class=&quot;highlighter-rouge&quot;&gt;type_of_material&lt;/code&gt; labelled as &lt;code class=&quot;highlighter-rouge&quot;&gt;News&lt;/code&gt;, then I’ve classified them with my clickbait detector. 
Here’s some titles classified as clickbait with the highest probability:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/clckbt_headlines.png&quot; width=&quot;600&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Very good! They all have the typical structure of the clickbait headlines like or &lt;em&gt;numbers at the begin&lt;/em&gt;, they contain catchy phrases as &lt;em&gt;that will make/keep you&lt;/em&gt;,  and so on. 
These are instead some of the titles with the lowest probability of being clickbait:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/no_clckbt_headlines.png&quot; width=&quot;600&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, also some headlines with the lowest probability start with a number, a feature usually associated to clickbait titles.
However it is clear these titles are not clickbaits.&lt;/p&gt;

&lt;p&gt;Ok, let’s see now how the number of clickbait titles varies in time 
This plot shows the ratio of clickbait over the whole number of articles published each month since 2008.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/clckbt_ratio_VS_time.png&quot; width=&quot;780&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The proportion of clickbait articles stays constant around 10% up to 2016, then it suddenly raises and in just 3 years it doubles.
In 2019 about 25% of news headlines were clickbait!&lt;/p&gt;

&lt;h3 id=&quot;web-vs-printed-version&quot;&gt;Web vs Printed Version&lt;/h3&gt;

&lt;p&gt;A confirmation of the fact that clickbait is mainly a web phenomenon can be found in looking at the printed version of the headlines that appear on the web page. It can be found, in the NYT Archive API under the attribute `print_headline.&lt;/p&gt;

&lt;p&gt;Web and print versions are often different, and the analysis shows that __
&lt;strong&gt;many times headlines in the printed version are not clickbait, while on the web they are&lt;/strong&gt;. Here some examples:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/clickbait_analysis/master/images/web_vs_print.png&quot; width=&quot;750&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Take a look at the notebook &lt;a href=&quot;https://github.com/LorBordin/clickbait_analysis/blob/master/NYT_analysis.ipynb&quot;&gt;NYT_analysis&lt;/a&gt; in my GitHub repo for an explanation on how I did the analysis and play with it.&lt;/p&gt;

&lt;h2 id=&quot;clickbait-on-uk-journals&quot;&gt;Clickbait on UK journals&lt;/h2&gt;

&lt;p&gt;To be finished …&lt;/p&gt;

&lt;!--- ### A closer look into The Guardian ---&gt;</content><author><name></name></author><summary type="html">Clickbaiting is the intenctional act of over-promising or otherwise misrepresenting what you’re going to find when you read a story on the web.</summary></entry><entry><title type="html">Building a Spam Classifier</title><link href="http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html" rel="alternate" type="text/html" title="Building a Spam Classifier" /><published>2019-12-19T00:00:00+00:00</published><updated>2019-12-19T00:00:00+00:00</updated><id>http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier</id><content type="html" xml:base="http://localhost:4000/ainulindale/jekyll/update/2019/12/19/Building-a-Spam-Classifier.html">&lt;p&gt;We all face the problem of spam e-mails in our inboxes.
Data show that the total volume of spam mails has been consistently growing in time to the point that nowadays it overtakes the amount of ordinary or &lt;em&gt;ham&lt;/em&gt; emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted &lt;sup id=&quot;fnref:foot&quot;&gt;&lt;a href=&quot;#fn:foot&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.
Most of the spam is already fitered out by our providers, but
still, quite a few spam emails are able to evade these filters and make their way to us.&lt;/p&gt;

&lt;p&gt;To better detect the unwanted spam the best way is probably to train an &lt;em&gt;ad hoc&lt;/em&gt; filter, tuned on our own mails. 
Contrary to what you could think this isn’t a hard task which can be realized with a little of &lt;strong&gt;machine learning&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In this post I will describe all the steps and analysis I’ve done, in order to build my personal spam-fitler.
Given that the important concepts are general and do not rely on any programming languag, here I do not show any code. 
If you’re interested in the code - written in &lt;strong&gt;Python&lt;/strong&gt; - you can it you can take a look at the Jupyter notebook I’ve used to create this post. Here’e the link to my &lt;a href=&quot;https://github.com/LorBordin/Spam_classifier&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;1-getting-the-data&quot;&gt;1. Getting the data&lt;/h2&gt;

&lt;p&gt;As previously said, to build a spam-filter &lt;strong&gt;tuned for our (or our company) needs&lt;/strong&gt;  one should use her/his own mails. Getting the training data in this way is easy, in my case I built up a quite large database in few months.&lt;/p&gt;

&lt;p&gt;However, instead of sharing my personal messages, to make this post I’ve opted for the &lt;a href=&quot;https://spamassassin.apache.org/old/publiccorpus/&quot;&gt;Apache SpamAssassin’s public datasets&lt;/a&gt;. It’s the perfect set to start with: no pre-processing at all has been made on it, the dataset consists in &lt;em&gt;raw&lt;/em&gt; emails, the  same we get from the mail-box.&lt;/p&gt;

&lt;p&gt;The SpamAssasins’s dataset consists in three different compressed files that contain:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2500 e-mails classified as &lt;strong&gt;easy ham&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;250 &lt;strong&gt;hard ham&lt;/strong&gt; e-mails,&lt;/li&gt;
  &lt;li&gt;500 &lt;strong&gt;spam&lt;/strong&gt; e-mails.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first thing we have to do before starting any analysis is to split our dataset into a &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; set.
In practice we label the emails with their class (ham or spam), then we merge the sets together, shuffle and finally split the dataset in two: a bigger one (80% of the mail) and a smaller set for the final evaluation (the test set).&lt;/p&gt;

&lt;h2 id=&quot;2-data-cleaning&quot;&gt;2. Data cleaning&lt;/h2&gt;

&lt;p&gt;Let’s focus on the train set. We need to extract some features to train our spam-filter. A good thing to do is to look at the email content. 
Spam mails often consists in advertising messages, so they might contain some common word as &lt;em&gt;money&lt;/em&gt;,  &lt;em&gt;opportunity&lt;/em&gt;, &lt;em&gt;loan&lt;/em&gt; or &lt;em&gt;mortage&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So we want to extract the words content from each mail and build 
a list with the most common spam mails.&lt;/p&gt;

&lt;p&gt;Let’s start with the words extraction part. To avoid repetitions it’s better to firstly parse the mails’ content doing the following operations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;remove all the html tags,&lt;/li&gt;
  &lt;li&gt;remove punctuation,&lt;/li&gt;
  &lt;li&gt;covert words in lower-case,&lt;/li&gt;
  &lt;li&gt;replace numbers with &lt;em&gt;NUMBER&lt;/em&gt; and urls with &lt;em&gt;URL&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you use &lt;em&gt;python&lt;/em&gt;, some packages like &lt;code class=&quot;highlighter-rouge&quot;&gt;email&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;regex&lt;/code&gt; and  &lt;code class=&quot;highlighter-rouge&quot;&gt;beautifulsoup&lt;/code&gt; are very useful for this purpose.&lt;/p&gt;

&lt;h2 id=&quot;3-data-analysis&quot;&gt;3. Data Analysis&lt;/h2&gt;

&lt;p&gt;We’ve extracted the words let’s &lt;strong&gt;count their frequency&lt;/strong&gt;.
It is convenient to build 2 different vocabularies that count the frequency hammy and spammy words.
In doing so don’t forget to &lt;strong&gt;remove the english common stopwords&lt;/strong&gt; like &lt;em&gt;the&lt;/em&gt;, &lt;em&gt;is&lt;/em&gt;, &lt;em&gt;at&lt;/em&gt;, …, which are not useful to understand the content of the mail.
Here’s the 20 most common ham and spam words in my train dataset:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/words_freq.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good! Notice Words like &lt;strong&gt;free&lt;/strong&gt;, &lt;strong&gt;money&lt;/strong&gt;, &lt;strong&gt;click&lt;/strong&gt; that appear often only in spam mails. They look like really spammy words!&lt;/p&gt;

&lt;p&gt;Let’s parse the &lt;strong&gt;mails’ subjcet&lt;/strong&gt;  as well: it will probably contain important info.
Let’s build a vocabulary that counts the most frequent spam’s subject words.
Here’s the 20 most common words:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/subj_spam.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve already collected a lot of interesting features. Still many other  info can be extractred from database!
For instance, many emails consist in multipart and, a closer analysis reveals that spam and ham have in general a different structure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/email_type.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s a lot of &lt;em&gt;plain text&lt;/em&gt; in the ham dataset, while &lt;em&gt;html&lt;/em&gt; is more frequent in the spam one. Good to know.&lt;/p&gt;

&lt;h2 id=&quot;4-preprocessing&quot;&gt;4. Preprocessing&lt;/h2&gt;

&lt;p&gt;It’s time to convert all the extracted features into frequency vectors. 
For each mail we create a long vector (507 entries!) that stores&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the frequency if the &lt;strong&gt;most frequent 200 ham words&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;the frequency of the &lt;strong&gt;most frequent 250 spam words&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;the frequency of the &lt;strong&gt;most frequent 50 subject words&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;the &lt;strong&gt;email’s type&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Better to &lt;strong&gt;rescale the features vector&lt;/strong&gt; in order to perform better on the  Support Vector Machine algoritims.&lt;/p&gt;

&lt;p&gt;To get an a-priori idea of how the emails dispose in the features space, we use the &lt;em&gt;t-SNE&lt;/em&gt; algorithm that reduces the dimensionality of the features vector while trying to keep similar instances close and dissimilar instances apart.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/clustering.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how ham clusters in several regions of the pain, while spam spreads mainly in the top-right part of the plot.&lt;/p&gt;

&lt;p&gt;Before training any model, let’s tackle the &lt;strong&gt;overfitting&lt;/strong&gt; problem. The features vector we’ve created has a large number of entries and many of them consist in words that appear both in the ham and the spam vocabularies, so there are many repeated entries. Traininig any algorithm with such features will, most probably, overfit the data causiung the algorithm will perform poorly on the new ones. 
To avoid this problem I’ve created a function that randomly selects only a  subset of features, taking care of removing all the repetitions.
The subset length is treated as a hyper-parameter that is tuned for each classifier during the training phase.&lt;/p&gt;

&lt;h2 id=&quot;5-training-the-classifiers&quot;&gt;5. Training the classifiers&lt;/h2&gt;

&lt;p&gt;Time to train our filter!&lt;/p&gt;

&lt;p&gt;But firstly.. What &lt;strong&gt;score&lt;/strong&gt; should the maximise? 
The simplest thing we can  think is the &lt;strong&gt;accuracy&lt;/strong&gt;, that just counts the number of wrong predictions&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/ainulindale/gh-pages/_images/accuracy.png&quot; width=&quot;225&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;But, this is &lt;strong&gt;not a good score&lt;/strong&gt;: our dataset consists of  only ~ 15% spam mails, so a &lt;em&gt;dumb filter&lt;/em&gt; that says everything is ham would score ~ 0.85 out of 1! &lt;br /&gt;
Moreover, accuracy considers &lt;em&gt;false positive&lt;/em&gt; (FP) and &lt;em&gt;false negative&lt;/em&gt; (FN) equally important. In other words it makes no distinction if ham is classified as spam (FP), or if spam is classified as ham (FN). But, of course, we care more that our spam filter does not filter out ham messages, better if it lets some spam pass instead.
So instead of accuracy it’s better to use a combination of two additional scores, &lt;strong&gt;precision&lt;/strong&gt; and &lt;strong&gt;recall&lt;/strong&gt;,&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/ainulindale/gh-pages/_images/prec_rec.png&quot; width=&quot;380&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;where TP and FP stand for &lt;em&gt;true positive&lt;/em&gt; and &lt;em&gt;false negative&lt;/em&gt; (respectively, spam and spam classified as ham). Ideally we want to tune our filter to have the maximum possible precision - 1.0 - while keeping recall the highest possible.
Therefore, we decide to maximise the &lt;strong&gt;F1 score&lt;/strong&gt;, that is the &lt;em&gt;harmonic mean&lt;/em&gt; of precision and recall.&lt;/p&gt;

&lt;p&gt;We chose the score, what about the classifier?&lt;br /&gt;
Shall we train Linear model? Support Vector Machine? Random Forest? Well let’s train them all and &lt;strong&gt;make a voting classifier&lt;/strong&gt;, most probably it’ll perform better than any individual classifier.
Let’s train the following algorithms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a &lt;strong&gt;Logistic regressor&lt;/strong&gt;, - a Stochastic Gradient Descent or &lt;strong&gt;SGD classifier&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;Random Forest classifier&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;K Neighbors classifier&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;a Support Vector Machine or &lt;strong&gt;SVM classifer&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As previously discussed, each classifier is trained on a different feature sets. In this way not only we lower the risk of overfitting, but also the algorithms will make more uncorrelated errors and the voting classifier will perform better.&lt;/p&gt;

&lt;p&gt;To get an idea of how the individual classifiers will generalise to an independent data set, &lt;em&gt;cross-validation&lt;/em&gt; is a good option. It’s a tequinique that consists in dividing the train set in some validation sets (5 is a good choice), and then evaluating the classifier on each validation set, after having trained it on the rest of the data.
Here’s my results:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/cv_performances.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Since the Random Forest classifier performs the best among the individual ones, its vote count twice on the voting classifier.&lt;/p&gt;

&lt;p&gt;A great quality of Random Forests is that they make it easy to measure the relative importance of each feature. The following plot shows the features that contributed more than 1% in the decision process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/feature_importances.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the most important feature is &lt;em&gt;click&lt;/em&gt; (8%) followed by &lt;em&gt;please&lt;/em&gt; and the email types &lt;em&gt;text/html&lt;/em&gt; and &lt;em&gt;text/plain&lt;/em&gt; (all around 5%).&lt;/p&gt;

&lt;h2 id=&quot;6-final-evaluation&quot;&gt;6. Final evaluation&lt;/h2&gt;

&lt;p&gt;We’ve fine-tuned and trained the algorithms, cross validated them on the train dataset and built a voting classifier. It only reamin to evaluate our filter on the test set in order to asses its performances.&lt;/p&gt;

&lt;p&gt;Of course do not forget to preprocess the test set  in the same as we did for the training set before! &lt;br /&gt;
Here’s the perfomances of the voting classifier on an unknown dataset.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
 &lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_voting.png&quot; width=&quot;200&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;It performs very well, confirming the results of the cross-validation test!
Let’s take a look at the &lt;strong&gt;confusion matrix&lt;/strong&gt; :&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
 &lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/confusion_matrix.png&quot; width=&quot;300&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Wow! It misclassified only 2 ham mails; at the same time it let passed 13 spam mails out of 100. Not bad!&lt;/p&gt;

&lt;p&gt;But let’s take a closer look at the misclassified ham mails:&lt;/p&gt;

&lt;p&gt;The first is an advertising email form Ryanair&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ryanair in partnership with Primary Insurance&lt;br /&gt;
offer excellent value travel insurance from&lt;br /&gt;
£7.00GBP/9.00 Euro per person for 31 day cover.&lt;/p&gt;

  &lt;p&gt;Annual travel insurance* from £45.00GBP/63.00 Euro,&lt;br /&gt;
includes 24 days winter sports cover !&lt;/p&gt;

  &lt;p&gt;Our travel insurance provides a high standard of cover.&lt;/p&gt;

  &lt;p&gt;Summary of Cover&lt;/p&gt;

  &lt;p&gt;Medical Expenses up to £2 million&lt;br /&gt;
Personal Liability  up to £2 million &lt;br /&gt;
Personal Effects &amp;amp; Baggage up to £750&lt;br /&gt;
Personal Accident Maximum Benefit £15,000&lt;/p&gt;

  &lt;p&gt;…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;while the second is written in Japanese (according to Google translate)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OpenText社&lt;br /&gt;
伊東様&lt;/p&gt;

  &lt;p&gt;いつもお世話になっております。 
安井@infocomです。&lt;/p&gt;

  &lt;p&gt;あるタスクリストに、適当なマイルストーンを複数、タスクを複数用意し&lt;br /&gt;
それぞれのタスクについては、存在する適当なマイルストーンに割り当てたものと&lt;br /&gt;
します。&lt;br /&gt;
あるマイルストーンを見ると、添付の絵にあるような画面が表示されるのですが&lt;br /&gt;
ここで丸で囲った部分（ 期間 ）は、何を意味しているのでしょうか？&lt;/p&gt;

  &lt;p&gt;つまり、３７日 、２７日&lt;br /&gt;
のそれぞれの意味（算出のされ方について教えてください。）&lt;/p&gt;

  &lt;p&gt;また「週末を除く」とありますが、週末を除かない設定方法はあるのでしょうか？&lt;/p&gt;

  &lt;p&gt;以上、ご回答の方よろしくお願いいたします。&lt;/p&gt;

  &lt;p&gt;–
+————————————–+
インフォコム(株)&lt;br /&gt;
ナレッジマネジメント本部&lt;br /&gt;
KMフロンティア部&lt;br /&gt;
コラボレーティブシステムグループ&lt;/p&gt;

  &lt;p&gt;〒101-0062&lt;br /&gt;
東京都千代田区神田駿河台3-11三井住友海上駿河台別館5F&lt;br /&gt;
安井  剛&lt;br /&gt;
E-mail: go@infocom.co.jp&lt;br /&gt;
TEL: 03-3518-3299&lt;br /&gt;
FAX: 03-3518-3055&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here, the distinction between ham and spam gets thin, personally I’d labelled both the emails as spam.&lt;/p&gt;

&lt;p&gt;Before concluding, let’s compare the performances of the individual classifiers on the test set:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/LorBordin/Spam_classifier/master/images/test_performances.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As expected &lt;strong&gt;the voting classifier scores better than any individual one!&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-conclusion&quot;&gt;7. Conclusion&lt;/h2&gt;

&lt;p&gt;In this post we’ve exploited a public dataset of raw emails and in order to make a &lt;strong&gt;simple yet powerful spam-filter&lt;/strong&gt; that detects most of the spam that evade the controls of our providers.&lt;/p&gt;

&lt;p&gt;Again, take a look at my 
&lt;a href=&quot;https://github.com/LorBordin/Spam_classifier&quot;&gt;GitHub repo&lt;/a&gt;
and in particular at the  &lt;strong&gt;Jupyter Notebook&lt;/strong&gt; in it, if you want to inspect the code I used to make my filter.&lt;/p&gt;

&lt;!--- comment on periodically update the dictionaries and re-train the voting classifier in order to keep the filter up-to-date with the new spam e-mails. 
This is a simple task and it isn't time consuming as well. ---&gt;

&lt;p&gt;Let’s comment about some possibile generalisations.
Here we’ve build a spam-filter, however the same ideas can applied to other similar tasks; the code as well could be used with little changes.&lt;/p&gt;

&lt;p&gt;Obvious examples are a &lt;strong&gt;Hate-speach detector&lt;/strong&gt;, that classifies profiles in a social network with little modifications, or a &lt;strong&gt;Clickbait recognizer&lt;/strong&gt;, that detects false advertisement from a website.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:foot&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Email_spam#Statistics_and_estimates&quot;&gt;Wikipedia/spam”&amp;gt;Wikipedia/spam repo&lt;/a&gt; &lt;a href=&quot;#fnref:foot&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">We all face the problem of spam e-mails in our inboxes. Data show that the total volume of spam mails has been consistently growing in time to the point that nowadays it overtakes the amount of ordinary or ham emails. Estimates tell us that 97% of all emails sent over the Internet in 2008 were unwanted 1. Most of the spam is already fitered out by our providers, but still, quite a few spam emails are able to evade these filters and make their way to us. To better detect the unwanted spam the best way is probably to train an ad hoc filter, tuned on our own mails. Contrary to what you could think this isn’t a hard task which can be realized with a little of machine learning. Wikipedia/spam”&amp;gt;Wikipedia/spam repo &amp;#8617;</summary></entry></feed>